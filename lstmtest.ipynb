{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pykrx import stock\n",
    "from pykrx import bond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>종가</th>\n",
       "      <th>거래량</th>\n",
       "      <th>등락률</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>날짜</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>25889</td>\n",
       "      <td>25889</td>\n",
       "      <td>24854</td>\n",
       "      <td>25698</td>\n",
       "      <td>1209037</td>\n",
       "      <td>0.760665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>26215</td>\n",
       "      <td>26247</td>\n",
       "      <td>24272</td>\n",
       "      <td>24630</td>\n",
       "      <td>1852188</td>\n",
       "      <td>-4.155965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>24790</td>\n",
       "      <td>24887</td>\n",
       "      <td>23721</td>\n",
       "      <td>23918</td>\n",
       "      <td>1371700</td>\n",
       "      <td>-2.890784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>24111</td>\n",
       "      <td>25016</td>\n",
       "      <td>24078</td>\n",
       "      <td>24727</td>\n",
       "      <td>1401706</td>\n",
       "      <td>3.382390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>24920</td>\n",
       "      <td>24985</td>\n",
       "      <td>23950</td>\n",
       "      <td>24144</td>\n",
       "      <td>1156997</td>\n",
       "      <td>-2.357747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   날짜     시가     고가     저가     종가      거래량       등락률\n",
       "날짜                                                                  \n",
       "2018-01-02 2018-01-02  25889  25889  24854  25698  1209037  0.760665\n",
       "2018-01-03 2018-01-03  26215  26247  24272  24630  1852188 -4.155965\n",
       "2018-01-04 2018-01-04  24790  24887  23721  23918  1371700 -2.890784\n",
       "2018-01-05 2018-01-05  24111  25016  24078  24727  1401706  3.382390\n",
       "2018-01-08 2018-01-08  24920  24985  23950  24144  1156997 -2.357747"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_samsung = stock.get_market_ohlcv(\"20180101\", \"20231012\", \"086520\")\n",
    "df = pd.DataFrame(df_samsung,columns=['날짜','시가','고가','저가','종가','거래량','등락률'])\n",
    "df['날짜']= df.index\n",
    "df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'samsung close price'}, xlabel='날짜'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('samsung close price')\n",
    "df['종가'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import itertools\n",
    "# combinations = list(itertools.product([1, 2, 3], repeat=3))\n",
    "# list_pattern = [''.join([dict_1[num] for num in combination]) for combination in combinations]\n",
    "# len(list_pattern)\n",
    "len(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "close_prices = df['종가']\n",
    "labels = []\n",
    "for i in range(1, len(close_prices)):\n",
    "    if close_prices[i] > close_prices[i-1]:\n",
    "        labels.append('상')\n",
    "    elif close_prices[i] == close_prices[i-1]:\n",
    "        labels.append('보')\n",
    "    else:\n",
    "        labels.append('하')\n",
    "result = ''.join(labels)\n",
    "list_result = []\n",
    "for i in range(0, len(result)-2):\n",
    "    list_result.append(result[i:i+3])\n",
    "dict_1 = {1: \"상\", 2: \"보\", 3: \"하\"}\n",
    "dict_1_inverse = {v: k for k, v in dict_1.items()}\n",
    "encoder = LabelEncoder()\n",
    "X = [[dict_1_inverse[char] for char in pattern] for pattern in list_result]\n",
    "X= [int(''.join(map(str,x))) for x in X]\n",
    "y = encoder.fit_transform(X[3:])  \n",
    "X_data = np.array(X[:-3])\n",
    "X_data = np.expand_dims(X_data, axis=-1)\n",
    "y_data = np.array(y)\n",
    "X_train, X_test, y_train, y_test = X_data[:993], X_data[993:], y_data[:993], y_data[993:]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# len(np.transpose(X_train)[0])\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.13443396226415094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 의사결정 트리 모델 생성 및 학습\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 예측 수행 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 50)                10400     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 25)                1275      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,675\n",
      "Trainable params: 11,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "32/32 [==============================] - 4s 7ms/step - loss: 2.9657 - accuracy: 0.1229\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.6390 - accuracy: 0.1188\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4990 - accuracy: 0.1158\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4539 - accuracy: 0.1088\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4381 - accuracy: 0.1178\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4308 - accuracy: 0.1198\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4250 - accuracy: 0.1098\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4246 - accuracy: 0.1188\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4229 - accuracy: 0.1118\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 2.4214 - accuracy: 0.1128\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4212 - accuracy: 0.1178\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 2.4231 - accuracy: 0.1158\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 2.4218 - accuracy: 0.1108\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4202 - accuracy: 0.1249\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4200 - accuracy: 0.1158\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4197 - accuracy: 0.1219\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4183 - accuracy: 0.1229\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4194 - accuracy: 0.1229\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4194 - accuracy: 0.1299\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 2.4195 - accuracy: 0.1138\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4176 - accuracy: 0.1279\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4196 - accuracy: 0.1229\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4175 - accuracy: 0.1158\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4180 - accuracy: 0.1118\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4183 - accuracy: 0.1239\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4165 - accuracy: 0.1229\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4161 - accuracy: 0.1269\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4169 - accuracy: 0.1188\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4169 - accuracy: 0.1198\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4176 - accuracy: 0.1158\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 2.4153 - accuracy: 0.1289\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4154 - accuracy: 0.1229\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4185 - accuracy: 0.1138\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4153 - accuracy: 0.1360\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 2.4170 - accuracy: 0.1229\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4154 - accuracy: 0.1208\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4156 - accuracy: 0.1178\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4168 - accuracy: 0.1178\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 2.4162 - accuracy: 0.1198\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.4164 - accuracy: 0.1219\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4164 - accuracy: 0.1138\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4165 - accuracy: 0.1078\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4149 - accuracy: 0.1289\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4176 - accuracy: 0.1249\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4148 - accuracy: 0.1178\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4151 - accuracy: 0.1178\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4144 - accuracy: 0.1239\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4157 - accuracy: 0.1178\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4143 - accuracy: 0.1269\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4151 - accuracy: 0.1269\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4133 - accuracy: 0.1208\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4156 - accuracy: 0.1249\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 2.4138 - accuracy: 0.1249\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4145 - accuracy: 0.1229\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4142 - accuracy: 0.1249\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4133 - accuracy: 0.1188\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4128 - accuracy: 0.1259\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4133 - accuracy: 0.1067\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4129 - accuracy: 0.1219\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4147 - accuracy: 0.1178\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4147 - accuracy: 0.1098\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4128 - accuracy: 0.1229\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4116 - accuracy: 0.1289\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4145 - accuracy: 0.1299\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4138 - accuracy: 0.1319\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.4134 - accuracy: 0.1239\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4145 - accuracy: 0.1118\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 2.4146 - accuracy: 0.1208\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4158 - accuracy: 0.1188\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4141 - accuracy: 0.1138\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4127 - accuracy: 0.1249\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4136 - accuracy: 0.1098\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4145 - accuracy: 0.1279\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4144 - accuracy: 0.1229\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4127 - accuracy: 0.1037\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4147 - accuracy: 0.1198\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 2.4121 - accuracy: 0.1178\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4167 - accuracy: 0.1118\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.4118 - accuracy: 0.1239\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 2.4133 - accuracy: 0.1198\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4120 - accuracy: 0.1229\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4122 - accuracy: 0.1299\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4130 - accuracy: 0.1188\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4110 - accuracy: 0.1249\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4138 - accuracy: 0.1178\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 2.4133 - accuracy: 0.1289\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 2.4134 - accuracy: 0.1208\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 2.4142 - accuracy: 0.0987\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4119 - accuracy: 0.1229\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4148 - accuracy: 0.1188\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4179 - accuracy: 0.1138\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.4142 - accuracy: 0.1128\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4124 - accuracy: 0.1128\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 2.4100 - accuracy: 0.1168\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4110 - accuracy: 0.1279\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4121 - accuracy: 0.1188\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4099 - accuracy: 0.1178\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4127 - accuracy: 0.1208\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.4127 - accuracy: 0.1208\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.4125 - accuracy: 0.1148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x179d9933640>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM 모델 구축 및 학습 \n",
    "num_classes = np.max(y_train) + 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(1, 1)))\n",
    "model.add(Dense(np.max(y_train) + 1, activation='leakyrelu')) \n",
    "model.summary()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 3s 11ms/step - loss: 2.6202 - accuracy: 0.1226\n",
      "## evaluation loss and_metrics ##\n",
      "[2.6202354431152344, 0.12264151126146317]\n",
      "14/14 [==============================] - 1s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '보보보',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '상상하',\n",
       " '보보보',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '상상하',\n",
       " '보보보',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '보보보',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '보보보',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '보보보',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '상상하',\n",
       " '보보보',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '상상하',\n",
       " '보보보',\n",
       " '보보보',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '보보보',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '하상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상상상',\n",
       " '상하상',\n",
       " '하하상',\n",
       " '하상상',\n",
       " '상하상']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 모델 평가\n",
    "loss_and_metrics = model.evaluate(X_test,y_test,batch_size=30)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 마지막 시퀀스를 기반으로 다음 패턴 예측하기 \n",
    "last_sequence = X_test\n",
    "predicted_sequence_encoded=model.predict(last_sequence)\n",
    "\n",
    "predicted_sequence=[]\n",
    "for prediction in predicted_sequence_encoded:\n",
    "    predicted_label=np.argmax(prediction) \n",
    "    predicted_label_string=encoder.inverse_transform([predicted_label])[0]\n",
    "    predicted_label_string=''.join([dict_1[int(digit)] for digit in str(predicted_label_string)])  \n",
    "    predicted_sequence.append(predicted_label_string)\n",
    "    \n",
    "predicted_sequence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pattern</th>\n",
       "      <th>Profit</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>하하상</td>\n",
       "      <td>-3.778504</td>\n",
       "      <td>2018-01-02 - 2018-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>하상하</td>\n",
       "      <td>-1.973203</td>\n",
       "      <td>2018-01-03 - 2018-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>상하하</td>\n",
       "      <td>0.271762</td>\n",
       "      <td>2018-01-04 - 2018-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>하하상</td>\n",
       "      <td>-2.094876</td>\n",
       "      <td>2018-01-05 - 2018-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>하상상</td>\n",
       "      <td>5.897946</td>\n",
       "      <td>2018-01-08 - 2018-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>상하상</td>\n",
       "      <td>-7.094595</td>\n",
       "      <td>2023-09-26 - 2023-10-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>하상상</td>\n",
       "      <td>-6.881243</td>\n",
       "      <td>2023-09-27 - 2023-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>상상하</td>\n",
       "      <td>-4.611650</td>\n",
       "      <td>2023-10-04 - 2023-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>상하상</td>\n",
       "      <td>2.181818</td>\n",
       "      <td>2023-10-05 - 2023-10-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>하상상</td>\n",
       "      <td>4.052443</td>\n",
       "      <td>2023-10-06 - 2023-10-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1420 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pattern    Profit                     Date\n",
       "0        하하상 -3.778504  2018-01-02 - 2018-01-05\n",
       "1        하상하 -1.973203  2018-01-03 - 2018-01-08\n",
       "2        상하하  0.271762  2018-01-04 - 2018-01-09\n",
       "3        하하상 -2.094876  2018-01-05 - 2018-01-10\n",
       "4        하상상  5.897946  2018-01-08 - 2018-01-11\n",
       "...      ...       ...                      ...\n",
       "1415     상하상 -7.094595  2023-09-26 - 2023-10-05\n",
       "1416     하상상 -6.881243  2023-09-27 - 2023-10-06\n",
       "1417     상상하 -4.611650  2023-10-04 - 2023-10-10\n",
       "1418     상하상  2.181818  2023-10-05 - 2023-10-11\n",
       "1419     하상상  4.052443  2023-10-06 - 2023-10-12\n",
       "\n",
       "[1420 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_profit = {}\n",
    "for i in range(0, len(result)-2):\n",
    "    pattern = ''.join(labels[i:i+3])\n",
    "    profit = (df['종가'][i+3]-df['종가'][i])/df['종가'][i]*100\n",
    "    date = df['날짜'][i].strftime('%Y-%m-%d') + ' - ' + df['날짜'][i+3].strftime('%Y-%m-%d')\n",
    "    dict_profit[i] = [pattern, profit, date]\n",
    "df_result = pd.DataFrame(dict_profit.values(), columns=['Pattern', 'Profit', 'Date'])\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lonec\\Documents\\FinalProject\\FinalProj\\lstmtest.ipynb 셀 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lonec/Documents/FinalProject/FinalProj/lstmtest.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mtype\u001b[39m(df_result[\u001b[39m'\u001b[39;49m\u001b[39mPattern\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\lonec\\anaconda3\\envs\\mecab\\lib\\site-packages\\pandas\\core\\generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5982\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5983\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5984\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5985\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5986\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5987\u001b[0m ):\n\u001b[0;32m   5988\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pattern</th>\n",
       "      <th>Profit</th>\n",
       "      <th>Date</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>하상하</td>\n",
       "      <td>-0.217738</td>\n",
       "      <td>2022-03-21 - 2022-03-24</td>\n",
       "      <td>상상하</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>상하하</td>\n",
       "      <td>-2.159019</td>\n",
       "      <td>2022-05-19 - 2022-05-24</td>\n",
       "      <td>상상하</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>상상보</td>\n",
       "      <td>10.363153</td>\n",
       "      <td>2023-01-20 - 2023-01-27</td>\n",
       "      <td>보보보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>하상상</td>\n",
       "      <td>3.475513</td>\n",
       "      <td>2023-01-30 - 2023-02-02</td>\n",
       "      <td>상상하</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>상상상</td>\n",
       "      <td>12.052117</td>\n",
       "      <td>2023-01-31 - 2023-02-03</td>\n",
       "      <td>보보보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>상하상</td>\n",
       "      <td>3.309930</td>\n",
       "      <td>2023-03-31 - 2023-04-05</td>\n",
       "      <td>상상하</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>하상상</td>\n",
       "      <td>4.504505</td>\n",
       "      <td>2023-04-03 - 2023-04-06</td>\n",
       "      <td>보보보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>상상상</td>\n",
       "      <td>21.894737</td>\n",
       "      <td>2023-04-04 - 2023-04-07</td>\n",
       "      <td>보보보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>하하상</td>\n",
       "      <td>-6.176084</td>\n",
       "      <td>2023-06-26 - 2023-06-29</td>\n",
       "      <td>보보보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>하하상</td>\n",
       "      <td>-2.022867</td>\n",
       "      <td>2023-08-11 - 2023-08-17</td>\n",
       "      <td>하하상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>하상하</td>\n",
       "      <td>-0.317209</td>\n",
       "      <td>2023-08-22 - 2023-08-25</td>\n",
       "      <td>상하상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>상하하</td>\n",
       "      <td>2.784603</td>\n",
       "      <td>2023-08-23 - 2023-08-28</td>\n",
       "      <td>하하상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>하하하</td>\n",
       "      <td>-14.240255</td>\n",
       "      <td>2023-08-31 - 2023-09-05</td>\n",
       "      <td>상하상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>하하하</td>\n",
       "      <td>-10.941476</td>\n",
       "      <td>2023-09-01 - 2023-09-06</td>\n",
       "      <td>하하상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>하상하</td>\n",
       "      <td>-4.301075</td>\n",
       "      <td>2023-09-12 - 2023-09-15</td>\n",
       "      <td>하상상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>상하상</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-09-13 - 2023-09-18</td>\n",
       "      <td>상상상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>하상하</td>\n",
       "      <td>-1.548673</td>\n",
       "      <td>2023-09-14 - 2023-09-19</td>\n",
       "      <td>상상상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>상하상</td>\n",
       "      <td>7.078652</td>\n",
       "      <td>2023-09-15 - 2023-09-20</td>\n",
       "      <td>상상상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>하상상</td>\n",
       "      <td>8.008899</td>\n",
       "      <td>2023-09-18 - 2023-09-21</td>\n",
       "      <td>하상상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>상상하</td>\n",
       "      <td>7.528090</td>\n",
       "      <td>2023-09-19 - 2023-09-22</td>\n",
       "      <td>상상상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>상하하</td>\n",
       "      <td>-7.660021</td>\n",
       "      <td>2023-09-20 - 2023-09-25</td>\n",
       "      <td>하상상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>하하상</td>\n",
       "      <td>-8.547889</td>\n",
       "      <td>2023-09-21 - 2023-09-26</td>\n",
       "      <td>상하상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>하상상</td>\n",
       "      <td>-5.851620</td>\n",
       "      <td>2023-09-22 - 2023-09-27</td>\n",
       "      <td>하하상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>상상하</td>\n",
       "      <td>-6.363636</td>\n",
       "      <td>2023-09-25 - 2023-10-04</td>\n",
       "      <td>하상상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>상하상</td>\n",
       "      <td>-7.094595</td>\n",
       "      <td>2023-09-26 - 2023-10-05</td>\n",
       "      <td>상상상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>하상상</td>\n",
       "      <td>-6.881243</td>\n",
       "      <td>2023-09-27 - 2023-10-06</td>\n",
       "      <td>상하상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>상상하</td>\n",
       "      <td>-4.611650</td>\n",
       "      <td>2023-10-04 - 2023-10-10</td>\n",
       "      <td>하하상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>상하상</td>\n",
       "      <td>2.181818</td>\n",
       "      <td>2023-10-05 - 2023-10-11</td>\n",
       "      <td>하상상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>하상상</td>\n",
       "      <td>4.052443</td>\n",
       "      <td>2023-10-06 - 2023-10-12</td>\n",
       "      <td>상하상</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pattern     Profit                     Date predict\n",
       "1036     하상하  -0.217738  2022-03-21 - 2022-03-24     상상하\n",
       "1078     상하하  -2.159019  2022-05-19 - 2022-05-24     상상하\n",
       "1246     상상보  10.363153  2023-01-20 - 2023-01-27     보보보\n",
       "1250     하상상   3.475513  2023-01-30 - 2023-02-02     상상하\n",
       "1251     상상상  12.052117  2023-01-31 - 2023-02-03     보보보\n",
       "1293     상하상   3.309930  2023-03-31 - 2023-04-05     상상하\n",
       "1294     하상상   4.504505  2023-04-03 - 2023-04-06     보보보\n",
       "1295     상상상  21.894737  2023-04-04 - 2023-04-07     보보보\n",
       "1350     하하상  -6.176084  2023-06-26 - 2023-06-29     보보보\n",
       "1384     하하상  -2.022867  2023-08-11 - 2023-08-17     하하상\n",
       "1390     하상하  -0.317209  2023-08-22 - 2023-08-25     상하상\n",
       "1391     상하하   2.784603  2023-08-23 - 2023-08-28     하하상\n",
       "1397     하하하 -14.240255  2023-08-31 - 2023-09-05     상하상\n",
       "1398     하하하 -10.941476  2023-09-01 - 2023-09-06     하하상\n",
       "1405     하상하  -4.301075  2023-09-12 - 2023-09-15     하상상\n",
       "1406     상하상   0.000000  2023-09-13 - 2023-09-18     상상상\n",
       "1407     하상하  -1.548673  2023-09-14 - 2023-09-19     상상상\n",
       "1408     상하상   7.078652  2023-09-15 - 2023-09-20     상상상\n",
       "1409     하상상   8.008899  2023-09-18 - 2023-09-21     하상상\n",
       "1410     상상하   7.528090  2023-09-19 - 2023-09-22     상상상\n",
       "1411     상하하  -7.660021  2023-09-20 - 2023-09-25     하상상\n",
       "1412     하하상  -8.547889  2023-09-21 - 2023-09-26     상하상\n",
       "1413     하상상  -5.851620  2023-09-22 - 2023-09-27     하하상\n",
       "1414     상상하  -6.363636  2023-09-25 - 2023-10-04     하상상\n",
       "1415     상하상  -7.094595  2023-09-26 - 2023-10-05     상상상\n",
       "1416     하상상  -6.881243  2023-09-27 - 2023-10-06     상하상\n",
       "1417     상상하  -4.611650  2023-10-04 - 2023-10-10     하하상\n",
       "1418     상하상   2.181818  2023-10-05 - 2023-10-11     하상상\n",
       "1419     하상상   4.052443  2023-10-06 - 2023-10-12     상하상"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predict = df_result[-423:].copy()\n",
    "df_predict['predict'] = predicted_sequence[:-1]\n",
    "# df_groupped = df_predict.groupby('predict')\n",
    "df_groupped.tail() #['Profit'].mean()\n",
    "# df_predict['predict'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 1],\n",
       "       [3, 1, 3],\n",
       "       [1, 3, 3],\n",
       "       ...,\n",
       "       [3, 1, 1],\n",
       "       [1, 1, 3],\n",
       "       [1, 3, 1]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(X) \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['하하상하하상',\n",
       " '하하상상상하',\n",
       " '상상하상상하',\n",
       " '상상하하상상',\n",
       " '하상상하하보',\n",
       " '하하보상하하',\n",
       " '상하하하하하',\n",
       " '하하하상하상',\n",
       " '상하상하하하',\n",
       " '하하하상상하',\n",
       " '상상하상하하',\n",
       " '상하하상상하',\n",
       " '상상하하하상',\n",
       " '하하상하상상',\n",
       " '하상상상상상',\n",
       " '상상상상하하',\n",
       " '상하하상하상',\n",
       " '상하상하상하',\n",
       " '하상하하하상',\n",
       " '하하상상상하',\n",
       " '상상하상하하',\n",
       " '상하하상상상',\n",
       " '상상상하상상',\n",
       " '하상상하하하',\n",
       " '하하하상하하',\n",
       " '상하하상하하',\n",
       " '상하하하보하',\n",
       " '하보하상상하',\n",
       " '상상하상상하',\n",
       " '상상하하상상',\n",
       " '하상상상상하',\n",
       " '상상하상상하',\n",
       " '상상하상하상',\n",
       " '상하상상하상',\n",
       " '상하상하하상',\n",
       " '하하상상하하',\n",
       " '상하하하상하',\n",
       " '하상하하상상',\n",
       " '하상상하하상',\n",
       " '하하상하상하',\n",
       " '하상하하상상',\n",
       " '하상상하하상',\n",
       " '하하상상상하',\n",
       " '상상하상상하',\n",
       " '상상하하상상',\n",
       " '하상상상상하',\n",
       " '상상하하상상',\n",
       " '하상상보하상',\n",
       " '보하상하하하',\n",
       " '하하하하하상',\n",
       " '하하상하하상',\n",
       " '하하상하상상',\n",
       " '하상상상하상',\n",
       " '상하상하상상',\n",
       " '하상상상하하',\n",
       " '상하하하하상',\n",
       " '하하상하상상',\n",
       " '하상상상상하',\n",
       " '상상하하상상',\n",
       " '하상상하하하',\n",
       " '하하하상하상',\n",
       " '상하상상하상',\n",
       " '상하상상상상',\n",
       " '상상상상상상',\n",
       " '상상상하하상',\n",
       " '하하상하하상',\n",
       " '하하상하하상',\n",
       " '하하상상상하',\n",
       " '상상하상하하',\n",
       " '상하하하하상',\n",
       " '하하상하상하',\n",
       " '하상하상하하',\n",
       " '상하하상상하',\n",
       " '상상하상하상',\n",
       " '상하상상하하',\n",
       " '상하하상하상',\n",
       " '상하상상상하',\n",
       " '상상하하상상',\n",
       " '하상상하상상',\n",
       " '하상상하하하',\n",
       " '하하하하하상',\n",
       " '하하상상하상',\n",
       " '상하상하하하',\n",
       " '하하하상상하',\n",
       " '상상하상하상',\n",
       " '상하상상하상',\n",
       " '상하상상하상',\n",
       " '상하상하상상',\n",
       " '하상상하하하',\n",
       " '하하하상상하',\n",
       " '상상하상상하',\n",
       " '상상하하하상',\n",
       " '하하상하상하',\n",
       " '하상하하하상',\n",
       " '하하상상상하',\n",
       " '상상하상하하',\n",
       " '상하하보하상',\n",
       " '보하상하상하',\n",
       " '하상하하하상',\n",
       " '하하상하하상',\n",
       " '하하상상상상',\n",
       " '상상상하상상',\n",
       " '하상상하상하',\n",
       " '하상하하하상',\n",
       " '하하상하상하',\n",
       " '하상하하하보',\n",
       " '하하보하상하',\n",
       " '하상하상하하',\n",
       " '상하하하하보',\n",
       " '하하보하상상',\n",
       " '하상상하상하',\n",
       " '하상하상상하',\n",
       " '상상하하상상',\n",
       " '하상상하상상',\n",
       " '하상상상상하',\n",
       " '상상하상하상',\n",
       " '상하상하상하',\n",
       " '하상하상하상',\n",
       " '상하상상하상',\n",
       " '상하상하하상',\n",
       " '하하상상상상',\n",
       " '상상상상하하',\n",
       " '상하하하하상',\n",
       " '하하상상상상',\n",
       " '상상상하하하',\n",
       " '하하하상상하',\n",
       " '상상하보하하',\n",
       " '보하하하하상',\n",
       " '하하상하하하',\n",
       " '하하하하상상',\n",
       " '하상상하상하',\n",
       " '하상하상하하',\n",
       " '상하하상상하',\n",
       " '상상하보하상',\n",
       " '보하상상상상',\n",
       " '상상상보상상',\n",
       " '보상상상하하',\n",
       " '상하하상하상',\n",
       " '상하상상하상',\n",
       " '상하상상하상',\n",
       " '상하상하하상',\n",
       " '하하상하상하',\n",
       " '하상하하하상',\n",
       " '하하상상하상',\n",
       " '상하상상하하',\n",
       " '상하하상하상',\n",
       " '상하상상하상',\n",
       " '상하상상하하',\n",
       " '상하하상하상',\n",
       " '상하상하하하',\n",
       " '하하하상하상',\n",
       " '상하상하상상',\n",
       " '하상상상하하',\n",
       " '상하하하상상',\n",
       " '하상상상하하',\n",
       " '상하하하상하',\n",
       " '하상하하보상',\n",
       " '하보상상하상',\n",
       " '상하상상상하',\n",
       " '상상하상하상',\n",
       " '상하상하하상',\n",
       " '하하상상상상',\n",
       " '상상상상하상',\n",
       " '상하상하하상',\n",
       " '하하상상하하',\n",
       " '상하하하상하',\n",
       " '하상하상하상',\n",
       " '상하상하하하',\n",
       " '하하하하하상',\n",
       " '하하상상상상',\n",
       " '상상상상상상',\n",
       " '상상상상상하',\n",
       " '상상하하하상',\n",
       " '하하상상하하',\n",
       " '상하하상하하',\n",
       " '상하하하상하',\n",
       " '하상하상보하',\n",
       " '상보하하상하',\n",
       " '하상하하하하',\n",
       " '하하하상하하',\n",
       " '상하하상하상',\n",
       " '상하상상하상',\n",
       " '상하상상상하',\n",
       " '상상하상상상',\n",
       " '상상상상상상',\n",
       " '상상상하하상',\n",
       " '하하상상상하',\n",
       " '상상하하하상',\n",
       " '하하상하상하',\n",
       " '하상하상하상',\n",
       " '상하상상상하',\n",
       " '상상하하상하',\n",
       " '하상하상상보',\n",
       " '상상보상상하',\n",
       " '상상하상상하',\n",
       " '상상하하하상',\n",
       " '하하상상하상',\n",
       " '상하상상하상',\n",
       " '상하상보보하',\n",
       " '보보하하상상',\n",
       " '하상상하상하',\n",
       " '하상하보보보',\n",
       " '보보보하하하',\n",
       " '하하하상상상',\n",
       " '상상상상하상',\n",
       " '상하상상하상',\n",
       " '상하상하상상',\n",
       " '하상상하상상',\n",
       " '하상상상하하',\n",
       " '상하하하상상',\n",
       " '하상상상하상',\n",
       " '상하상하상상',\n",
       " '하상상상상상',\n",
       " '상상상상하하',\n",
       " '상하하하상하',\n",
       " '하상하상상상',\n",
       " '상상상보하상',\n",
       " '보하상상하상',\n",
       " '상하상상상상',\n",
       " '상상상하상상',\n",
       " '하상상하보상',\n",
       " '하보상하상상',\n",
       " '하상상하하하',\n",
       " '하하하하하상',\n",
       " '하하상상상상',\n",
       " '상상상상상하',\n",
       " '상상하하하상',\n",
       " '하하상상하하',\n",
       " '상하하상하하',\n",
       " '상하하하상상',\n",
       " '하상상상하상',\n",
       " '상하상상하상',\n",
       " '상하상하하하',\n",
       " '하하하상상상',\n",
       " '상상상하보상',\n",
       " '하보상상상하',\n",
       " '상상하보하상',\n",
       " '보하상상상하',\n",
       " '상상하하하상',\n",
       " '하하상상하하',\n",
       " '상하하보상하',\n",
       " '보상하상하상',\n",
       " '상하상하하하',\n",
       " '하하하상상상',\n",
       " '상상상상하상',\n",
       " '상하상하하상',\n",
       " '하하상상상하',\n",
       " '상상하하상하',\n",
       " '하상하하상보',\n",
       " '하상보하하상',\n",
       " '하하상하상하',\n",
       " '하상하하상하',\n",
       " '하상하하상하',\n",
       " '하상하상상상',\n",
       " '상상상상하하',\n",
       " '상하하하상상',\n",
       " '하상상하하상',\n",
       " '하하상하상상',\n",
       " '하상상상하하',\n",
       " '상하하하하상',\n",
       " '하하상상하하',\n",
       " '상하하하상상',\n",
       " '하상상하하하',\n",
       " '하하하상하하',\n",
       " '상하하상상상',\n",
       " '상상상상상상',\n",
       " '상상상하하상',\n",
       " '하하상상하하',\n",
       " '상하하하상상',\n",
       " '하상상상상상',\n",
       " '상상상상하상',\n",
       " '상하상상보보',\n",
       " '상보보보보보',\n",
       " '보보보보보보',\n",
       " '보보보보보보',\n",
       " '보보보보보보',\n",
       " '보보보보보보',\n",
       " '보보보보보상',\n",
       " '보보상하하하',\n",
       " '하하하상상하',\n",
       " '상상하하하상',\n",
       " '하하상하상하',\n",
       " '하상하상상상',\n",
       " '상상상하하하',\n",
       " '하하하상상하',\n",
       " '상상하상하상',\n",
       " '상하상하상상',\n",
       " '하상상상하상',\n",
       " '상하상상상상',\n",
       " '상상상상상상',\n",
       " '상상상하상하',\n",
       " '하상하상상상',\n",
       " '상상상하상하',\n",
       " '하상하하상하',\n",
       " '하상하상상상',\n",
       " '상상상상상상',\n",
       " '상상상하하상',\n",
       " '하하상하상상',\n",
       " '하상상하상하',\n",
       " '하상하하상상',\n",
       " '하상상하하상',\n",
       " '하하상하하하',\n",
       " '하하하상상하',\n",
       " '상상하상상상',\n",
       " '상상상하하상',\n",
       " '하하상하하상',\n",
       " '하하상하하상',\n",
       " '하하상하상하',\n",
       " '하상하상상상',\n",
       " '상상상보하상',\n",
       " '보하상하하상',\n",
       " '하하상상하하',\n",
       " '상하하하상상',\n",
       " '하상상하상상',\n",
       " '하상상상하상',\n",
       " '상하상하상하',\n",
       " '하상하하하상',\n",
       " '하하상상하하',\n",
       " '상하하하상하',\n",
       " '하상하하하하',\n",
       " '하하하하하상',\n",
       " '하하상하하상',\n",
       " '하하상하하상',\n",
       " '하하상하하하',\n",
       " '하하하하상상',\n",
       " '하상상상상상',\n",
       " '상상상하상하',\n",
       " '하상하하하하',\n",
       " '하하하하하상',\n",
       " '하하상하하상',\n",
       " '하하상상하상',\n",
       " '상하상하하상',\n",
       " '하하상하하하',\n",
       " '하하하하상하',\n",
       " '하상하상상하',\n",
       " '상상하하상하',\n",
       " '하상하상하상',\n",
       " '상하상상상상',\n",
       " '상상상하하상',\n",
       " '하하상하상상',\n",
       " '하상상상상보',\n",
       " '상상보하하상',\n",
       " '하하상하하하',\n",
       " '하하하상상보',\n",
       " '상상보상하상',\n",
       " '상하상하하하',\n",
       " '하하하상하상',\n",
       " '상하상하상상',\n",
       " '하상상하하상',\n",
       " '하하상하하상',\n",
       " '하하상하하하',\n",
       " '하하하상하하',\n",
       " '상하하하하하',\n",
       " '하하하하하상',\n",
       " '하하상상상상',\n",
       " '상상상하하하',\n",
       " '하하하상하상',\n",
       " '상하상상상보',\n",
       " '상상보상상하',\n",
       " '상상하하하하',\n",
       " '하하하상상상',\n",
       " '상상상하하하',\n",
       " '하하하하상하',\n",
       " '하상하하상하',\n",
       " '하상하상상하',\n",
       " '상상하하하하',\n",
       " '하하하상상상',\n",
       " '상상상하하하',\n",
       " '하하하하상하',\n",
       " '하상하상상하',\n",
       " '상상하하상상',\n",
       " '하상상상하상',\n",
       " '상하상상하상',\n",
       " '상하상상하상',\n",
       " '상하상하하상',\n",
       " '하하상상상상',\n",
       " '상상상상상상',\n",
       " '상상상하하하',\n",
       " '하하하상하상',\n",
       " '상하상하하하',\n",
       " '하하하상상하',\n",
       " '상상하하상상',\n",
       " '하상상하하하',\n",
       " '하하하상상상',\n",
       " '상상상상상상',\n",
       " '상상상하하하',\n",
       " '하하하상상하',\n",
       " '상상하하상하',\n",
       " '하상하하상상',\n",
       " '하상상하상하',\n",
       " '하상하하상상',\n",
       " '하상상상상상',\n",
       " '상상상상하상',\n",
       " '상하상상하하',\n",
       " '상하하상하상',\n",
       " '상하상하하상',\n",
       " '하하상하상하',\n",
       " '하상하하하상',\n",
       " '하하상상하하',\n",
       " '상하하하하하',\n",
       " '하하하하상상',\n",
       " '하상상하보상',\n",
       " '하보상상상하',\n",
       " '상상하하하하',\n",
       " '하하하하하하',\n",
       " '하하하하상하',\n",
       " '하상하하하상',\n",
       " '하하상상상하',\n",
       " '상상하상상하',\n",
       " '상상하하상상',\n",
       " '하상상보하하',\n",
       " '보하하상하상',\n",
       " '상하상하하상',\n",
       " '하하상하보하',\n",
       " '하보하상상상',\n",
       " '상상상보상하',\n",
       " '보상하상상상',\n",
       " '상상상상상상',\n",
       " '상상상상하상',\n",
       " '상하상상상상',\n",
       " '상상상하상상',\n",
       " '하상상하상하',\n",
       " '하상하상상하',\n",
       " '상상하상상하',\n",
       " '상상하상하하',\n",
       " '상하하상상상',\n",
       " '상상상하하상',\n",
       " '하하상상상상',\n",
       " '상상상상하하',\n",
       " '상하하상보보',\n",
       " '상보보상하상',\n",
       " '상하상상상상',\n",
       " '상상상상하하',\n",
       " '상하하상상상',\n",
       " '상상상하하하',\n",
       " '하하하상하상',\n",
       " '상하상상상상',\n",
       " '상상상하상하',\n",
       " '하상하하하하',\n",
       " '하하하하하상',\n",
       " '하하상상하하',\n",
       " '상하하상상하',\n",
       " '상상하하하상',\n",
       " '하하상상하상',\n",
       " '상하상하상하',\n",
       " '하상하상상상',\n",
       " '상상상하상상',\n",
       " '하상상상하하',\n",
       " '상하하보하상',\n",
       " '보하상하하상',\n",
       " '하하상상상하',\n",
       " '상상하상하상',\n",
       " '상하상하상하',\n",
       " '하상하상상상',\n",
       " '상상상상하하',\n",
       " '상하하상상상',\n",
       " '상상상하하상',\n",
       " '하하상상상하',\n",
       " '상상하상하하',\n",
       " '상하하상상상',\n",
       " '상상상하하하',\n",
       " '하하하상하상',\n",
       " '상하상상하상',\n",
       " '상하상하하하',\n",
       " '하하하상상하',\n",
       " '상상하하하하',\n",
       " '하하하하상하',\n",
       " '하상하하하상',\n",
       " '하하상하상하',\n",
       " '하상하상상하',\n",
       " '상상하하상상',\n",
       " '하상상하상상']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#여기부터\n",
    "list_test = []\n",
    "for i in range(0, len(result)-2):\n",
    "    list_test.append(result[i:i+3])\n",
    "print(len(list_test))\n",
    "list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12/12 [==============================] - 7s 9ms/step - loss: 2.0807 - accuracy: 0.3227\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 1.0229 - accuracy: 0.3050\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.8720 - accuracy: 0.3129\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.8868 - accuracy: 0.3245\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2760 - accuracy: 0.3147\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3.3780 - accuracy: 0.3174\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5.4037 - accuracy: 0.3200\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 4.4949 - accuracy: 0.3298\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 5.1837 - accuracy: 0.3351\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 5.5620 - accuracy: 0.3351\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 5.7287 - accuracy: 0.3351\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 5.7276 - accuracy: 0.3351\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5.7267 - accuracy: 0.3351\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5.6542 - accuracy: 0.3351\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5.6247 - accuracy: 0.3351\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5.5522 - accuracy: 0.3351\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 5.4407 - accuracy: 0.3351\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 6.4808 - accuracy: 0.3351\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.8735 - accuracy: 0.3351\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.3376 - accuracy: 0.3351\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.4953 - accuracy: 0.3351\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 7.5373 - accuracy: 0.3351\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 7.5073 - accuracy: 0.3351\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.5061 - accuracy: 0.3351\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 7.5050 - accuracy: 0.3351\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.4897 - accuracy: 0.3351\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 7.4604 - accuracy: 0.3351\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 7.3743 - accuracy: 0.3351\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 7.3024 - accuracy: 0.3351\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 7.2164 - accuracy: 0.3351\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 7.2162 - accuracy: 0.3351\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 7.2305 - accuracy: 0.3351\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 7.2590 - accuracy: 0.3351\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 7.2447 - accuracy: 0.3351\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 7.2304 - accuracy: 0.3351\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 7.2161 - accuracy: 0.3351\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 7.2161 - accuracy: 0.3351\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 7.1732 - accuracy: 0.3351\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 7.1589 - accuracy: 0.3351\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 7.1731 - accuracy: 0.3351\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 7.1588 - accuracy: 0.3351\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 7.1588 - accuracy: 0.3351\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 7.1588 - accuracy: 0.3351\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.1588 - accuracy: 0.3351\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.1588 - accuracy: 0.3351\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 7.1588 - accuracy: 0.3351\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 7.1588 - accuracy: 0.3351\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.1588 - accuracy: 0.3351\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 7.1588 - accuracy: 0.3351\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 7.1588 - accuracy: 0.3351\n",
      "4/4 [==============================] - 1s 4ms/step - loss: 7.6350 - accuracy: 0.3509\n",
      "## evaluation loss and_metrics ##\n",
      "[7.634972095489502, 0.35087719559669495]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "다음 패턴: 상하보\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 문자열 라벨을 숫자로 변환\n",
    "encoder = LabelEncoder()\n",
    "labels_encoded = encoder.fit_transform(list(result))\n",
    "\n",
    "# 숫자 라벨을 원-핫 인코딩으로 변환 (3개 클래스: '상', '보', '하')\n",
    "labels_onehot = to_categorical(labels_encoded)\n",
    "\n",
    "# 학습 데이터와 타겟 데이터 설정 (시퀀스 길이: 6)\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range(0, len(labels_onehot) - 9, 3):\n",
    "    X_data.append(labels_onehot[i:i+6])\n",
    "    y_data.append(labels_onehot[i+6:i+9])\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "# 학습 세트와 테스트 세트로 분할 (비율은 80:20으로 설정)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "\n",
    "# LSTM 모델 구축 및 학습 \n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(y_train.shape[1] * y_train.shape[2]))\n",
    "model.add(tf.keras.layers.Reshape((y_train.shape[1], y_train.shape[2])))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50)\n",
    "\n",
    "# 모델 평가\n",
    "loss_and_metrics = model.evaluate(X_test,y_test,batch_size=30)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 마지막 시퀀스를 기반으로 다음 패턴 예측하기 \n",
    "last_sequence_encoded = labels_onehot[-9:-3].reshape((1,X_test.shape[1],X_test.shape[2]))\n",
    "predicted_sequence_encoded=model.predict(last_sequence_encoded)[0]\n",
    "\n",
    "predicted_sequence=[]\n",
    "for prediction in predicted_sequence_encoded:\n",
    "    predicted_label=np.argmax(prediction)  \n",
    "    predicted_label_string=encoder.inverse_transform([predicted_label])[0]  \n",
    "    predicted_sequence.append(predicted_label_string)\n",
    "    \n",
    "print(\"다음 패턴:\", \"\".join(predicted_sequence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "31/31 [==============================] - 5s 4ms/step - loss: 3.0676 - accuracy: 0.0837\n",
      "Epoch 2/30\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.7174 - accuracy: 0.1220\n",
      "Epoch 3/30\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.4872 - accuracy: 0.1341\n",
      "Epoch 4/30\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.3948 - accuracy: 0.1300\n",
      "Epoch 5/30\n",
      "31/31 [==============================] - 0s 7ms/step - loss: 2.3704 - accuracy: 0.1361\n",
      "Epoch 6/30\n",
      "31/31 [==============================] - 0s 6ms/step - loss: 2.3628 - accuracy: 0.1321\n",
      "Epoch 7/30\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.3584 - accuracy: 0.1300\n",
      "Epoch 8/30\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.3546 - accuracy: 0.1381\n",
      "Epoch 9/30\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.3524 - accuracy: 0.1250\n",
      "Epoch 10/30\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.3487 - accuracy: 0.1704\n",
      "Epoch 11/30\n",
      "31/31 [==============================] - 0s 6ms/step - loss: 2.3468 - accuracy: 0.1300\n",
      "Epoch 12/30\n",
      "31/31 [==============================] - 0s 7ms/step - loss: 2.3439 - accuracy: 0.1411\n",
      "Epoch 13/30\n",
      "31/31 [==============================] - 0s 8ms/step - loss: 2.3404 - accuracy: 0.1270\n",
      "Epoch 14/30\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.3387 - accuracy: 0.1442\n",
      "Epoch 15/30\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.3370 - accuracy: 0.1371\n",
      "Epoch 16/30\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.3355 - accuracy: 0.1391\n",
      "Epoch 17/30\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.3340 - accuracy: 0.1401\n",
      "Epoch 18/30\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 2.3312 - accuracy: 0.1240\n",
      "Epoch 19/30\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.3273 - accuracy: 0.1270\n",
      "Epoch 20/30\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.3261 - accuracy: 0.1421\n",
      "Epoch 21/30\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.3239 - accuracy: 0.1270\n",
      "Epoch 22/30\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.3221 - accuracy: 0.1290\n",
      "Epoch 23/30\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.3199 - accuracy: 0.1210\n",
      "Epoch 24/30\n",
      "31/31 [==============================] - 0s 6ms/step - loss: 2.3186 - accuracy: 0.1270\n",
      "Epoch 25/30\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.3160 - accuracy: 0.1270\n",
      "Epoch 26/30\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.3152 - accuracy: 0.1270\n",
      "Epoch 27/30\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.3122 - accuracy: 0.1270\n",
      "Epoch 28/30\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.3113 - accuracy: 0.1411\n",
      "Epoch 29/30\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.3082 - accuracy: 0.1270\n",
      "Epoch 30/30\n",
      "31/31 [==============================] - 0s 6ms/step - loss: 2.3094 - accuracy: 0.1270\n",
      "15/15 [==============================] - 2s 7ms/step - loss: 2.3457 - accuracy: 0.1174\n",
      "## evaluation loss and_metrics ##\n",
      "[2.345700740814209, 0.11737088859081268]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "다음 패턴: 상하보\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 각 6글자 문자열을 숫자로 변환\n",
    "encoder = LabelEncoder()\n",
    "labels_encoded = encoder.fit_transform(list_result)\n",
    "\n",
    "# 학습 데이터와 타겟 데이터 설정 (윈도우 길이: 1)\n",
    "X_data = labels_encoded[:-1]\n",
    "y_data = labels_encoded[1:]\n",
    "\n",
    "# LSTM 모델은 3차원 입력 데이터를 필요로 하므로 reshape 함수 사용 \n",
    "X_data = X_data.reshape((X_data.shape[0], 1, 1))\n",
    "\n",
    "# 원-핫 인코딩으로 변환 (클래스 수: len(np.unique(labels_encoded)))\n",
    "y_data = to_categorical(y_data, num_classes=len(np.unique(labels_encoded)))\n",
    "\n",
    "# 학습 세트와 테스트 세트로 분할 (비율은 70:30으로 설정)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,test_size=0.3)\n",
    "\n",
    "# LSTM 모델 구축 및 학습 \n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax')) # 출력 레이어의 뉴런 수는 전체 클래스 수와 동일해야 함.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=30)\n",
    "\n",
    "# 모델 평가\n",
    "loss_and_metrics=model.evaluate(X_test,y_test,batch_size=30)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 마지막 시퀀스를 기반으로 다음 패턴 예측하기 \n",
    "last_sequence_encoded = X_data[-1].reshape((1, X_test.shape[1], X_test.shape[2]))\n",
    "predicted_sequence_encoded=model.predict(last_sequence_encoded)[0]\n",
    "\n",
    "predicted_label=np.argmax(predicted_sequence_encoded)  \n",
    "predicted_label_string=encoder.inverse_transform([predicted_label])  \n",
    "\n",
    "print(\"다음 패턴:\",\"\".join(predicted_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.014619903536049791]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y[0]\n",
    "# ['시가', '고가', '저가', '종가', '거래량', '등락률']\n",
    "# print(\"Tomorrow's SEC price :\", raw_df['종가'][-1] * pred_y[-1] / dfy['종가'][-1], 'KRW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7244819646967979, 0.6445743989603221, 0.7576489533010663, 0.08256334701612693, 0.7492217697310093], [0.7352264006139114, 0.6361273554255598, 0.7463768115941428, 0.05536764923932509, 0.7383183684149697], [0.7429009976975639, 0.6400259909031424, 0.7085346215780428, 0.0906821193282697, 0.6783496611767513], [0.6753645433614217, 0.5932423651721512, 0.706924315619911, 0.06409628563533726, 0.6845801762144883], [0.6753645433614217, 0.5938921377517483, 0.7020933977455152, 0.04778335676619261, 0.6947047631508109], [0.6830391404450743, 0.5815464587394035, 0.6618357487922173, 0.08390599030370034, 0.6347360559125925], [0.6515732924020989, 0.5601039636126992, 0.6626409017712832, 0.053886431972231245, 0.6355148702923097], [0.6308518802762372, 0.5536062378167282, 0.6618357487922173, 0.0354487482750999, 0.6464182716083494], [0.6477359938602727, 0.5497076023391456, 0.6264090177133151, 0.0754508989432615, 0.605141109483342], [0.6193399846507583, 0.5549057829759224, 0.6409017713365024, 0.07316955551841672, 0.6495335291272178]] -> [0.6728979605187315]\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10, 10)            640       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10, 10)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 10)                840       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,491\n",
      "Trainable params: 1,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "31/31 [==============================] - 5s 19ms/step - loss: 3.1395e-04 - accuracy: 0.0010\n",
      "Epoch 2/60\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 2.1006e-04 - accuracy: 0.0010\n",
      "Epoch 3/60\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 1.2128e-04 - accuracy: 0.0010\n",
      "Epoch 4/60\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 7.8965e-05 - accuracy: 0.0010\n",
      "Epoch 5/60\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 7.3078e-05 - accuracy: 0.0010\n",
      "Epoch 6/60\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 6.1987e-05 - accuracy: 0.0010\n",
      "Epoch 7/60\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 5.6926e-05 - accuracy: 0.0010\n",
      "Epoch 8/60\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 4.3631e-05 - accuracy: 0.0010\n",
      "Epoch 9/60\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 4.8460e-05 - accuracy: 0.0010\n",
      "Epoch 10/60\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 4.4184e-05 - accuracy: 0.0010\n",
      "Epoch 11/60\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 4.8713e-05 - accuracy: 0.0010\n",
      "Epoch 12/60\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 4.0360e-05 - accuracy: 0.0010\n",
      "Epoch 13/60\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 3.9712e-05 - accuracy: 0.0010\n",
      "Epoch 14/60\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 4.0125e-05 - accuracy: 0.0010\n",
      "Epoch 15/60\n",
      "31/31 [==============================] - 0s 14ms/step - loss: 4.8417e-05 - accuracy: 0.0010\n",
      "Epoch 16/60\n",
      "31/31 [==============================] - 0s 14ms/step - loss: 3.5608e-05 - accuracy: 0.0010\n",
      "Epoch 17/60\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 4.5219e-05 - accuracy: 0.0010\n",
      "Epoch 18/60\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 4.4223e-05 - accuracy: 0.0010\n",
      "Epoch 19/60\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 3.2985e-05 - accuracy: 0.0010\n",
      "Epoch 20/60\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 3.6060e-05 - accuracy: 0.0010\n",
      "Epoch 21/60\n",
      "31/31 [==============================] - 0s 14ms/step - loss: 3.2828e-05 - accuracy: 0.0010\n",
      "Epoch 22/60\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 3.6992e-05 - accuracy: 0.0010\n",
      "Epoch 23/60\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 3.9367e-05 - accuracy: 0.0010\n",
      "Epoch 24/60\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 3.9090e-05 - accuracy: 0.0010\n",
      "Epoch 25/60\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 3.2508e-05 - accuracy: 0.0010\n",
      "Epoch 26/60\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 3.5153e-05 - accuracy: 0.0010\n",
      "Epoch 27/60\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 3.4003e-05 - accuracy: 0.0010\n",
      "Epoch 28/60\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 3.4645e-05 - accuracy: 0.0010\n",
      "Epoch 29/60\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 3.6047e-05 - accuracy: 0.0010\n",
      "Epoch 30/60\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 2.9627e-05 - accuracy: 0.0010\n",
      "Epoch 31/60\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 2.8168e-05 - accuracy: 0.0010\n",
      "Epoch 32/60\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 3.1886e-05 - accuracy: 0.0010\n",
      "Epoch 33/60\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 3.8530e-05 - accuracy: 0.0010\n",
      "Epoch 34/60\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 3.8858e-05 - accuracy: 0.0010\n",
      "Epoch 35/60\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 3.8258e-05 - accuracy: 0.0010\n",
      "Epoch 36/60\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 3.5272e-05 - accuracy: 0.0010\n",
      "Epoch 37/60\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 3.2179e-05 - accuracy: 0.0010\n",
      "Epoch 38/60\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 3.4561e-05 - accuracy: 0.0010\n",
      "Epoch 39/60\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 3.2113e-05 - accuracy: 0.0010\n",
      "Epoch 40/60\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 3.4784e-05 - accuracy: 0.0010\n",
      "Epoch 41/60\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 3.2991e-05 - accuracy: 0.0010\n",
      "Epoch 42/60\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 3.0783e-05 - accuracy: 0.0010\n",
      "Epoch 43/60\n",
      "31/31 [==============================] - 1s 26ms/step - loss: 3.7272e-05 - accuracy: 0.0010\n",
      "Epoch 44/60\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.4425e-05 - accuracy: 0.0010\n",
      "Epoch 45/60\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 2.6278e-05 - accuracy: 0.0010\n",
      "Epoch 46/60\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 3.0663e-05 - accuracy: 0.0010\n",
      "Epoch 47/60\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 2.7491e-05 - accuracy: 0.0010\n",
      "Epoch 48/60\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 3.4343e-05 - accuracy: 0.0010\n",
      "Epoch 49/60\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 2.6232e-05 - accuracy: 0.0010\n",
      "Epoch 50/60\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 2.6314e-05 - accuracy: 0.0010\n",
      "Epoch 51/60\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 3.2493e-05 - accuracy: 0.0010\n",
      "Epoch 52/60\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 2.5349e-05 - accuracy: 0.0010\n",
      "Epoch 53/60\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 2.7603e-05 - accuracy: 0.0010\n",
      "Epoch 54/60\n",
      "31/31 [==============================] - 1s 24ms/step - loss: 2.8558e-05 - accuracy: 0.0010\n",
      "Epoch 55/60\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 3.7254e-05 - accuracy: 0.0010\n",
      "Epoch 56/60\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 3.0682e-05 - accuracy: 0.0010\n",
      "Epoch 57/60\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 3.1694e-05 - accuracy: 0.0010\n",
      "Epoch 58/60\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 3.0357e-05 - accuracy: 0.0010\n",
      "Epoch 59/60\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 3.3496e-05 - accuracy: 0.0010\n",
      "Epoch 60/60\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 2.3864e-05 - accuracy: 0.0010\n",
      "14/14 [==============================] - 1s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "#LSTM 예제\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pykrx import stock\n",
    "\n",
    "\n",
    "raw_df = stock.get_market_ohlcv(\"20180101\", \"20231012\", \"086520\")\n",
    "\n",
    "window_size = 10 \n",
    "data_size = 5\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    \"\"\"최솟값과 최댓값을 이용하여 0 ~ 1 값으로 변환\"\"\"\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # 0으로 나누기 에러가 발생하지 않도록 매우 작은 값(1e-7)을 더해서 나눔\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "dfx = raw_df[['시가','고가','저가','거래량', '종가']]\n",
    "dfx = MinMaxScaler(dfx)\n",
    "dfy = dfx[['종가']]\n",
    "\n",
    "x = dfx.values.tolist()\n",
    "y = dfy.values.tolist()\n",
    "\n",
    "data_x = []\n",
    "data_y = []\n",
    "for i in range(len(y) - window_size):\n",
    "    _x = x[i : i + window_size] # 다음 날 종가(i+windows_size)는 포함되지 않음\n",
    "    _y = y[i + window_size]     # 다음 날 종가\n",
    "    data_x.append(_x)\n",
    "    data_y.append(_y)\n",
    "print(_x, \"->\", _y)\n",
    "\n",
    "train_size = int(len(data_y) * 0.7)\n",
    "train_x = np.array(data_x[0 : train_size])\n",
    "train_y = np.array(data_y[0 : train_size])\n",
    "\n",
    "test_size = len(data_y) - train_size\n",
    "test_x = np.array(data_x[train_size : len(data_x)])\n",
    "test_y = np.array(data_y[train_size : len(data_y)])\n",
    "\n",
    "# 모델 생성\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=10, activation='relu', return_sequences=True, input_shape=(window_size, data_size)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(units=10, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=1))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error',metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, epochs=60)\n",
    "pred_y = model.predict(test_x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# Visualising the results\n",
    "plt.figure()\n",
    "plt.plot(test_y, color='red', label='real SEC stock price')\n",
    "plt.plot(pred_y, color='blue', label='predicted SEC stock price')\n",
    "plt.title('SEC stock price prediction')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('stock price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# raw_df.close[-1] : dfy.close[-1] = x : pred_y[-1]\n",
    "print(\"Tomorrow's SEC price :\", raw_df.close[-1] * pred_y[-1] / dfy.close[-1], 'KRW')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
